# -*- coding: utf-8 -*-
"""indrive_ds_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v_rB_VMiJ-w35vBVykIX_8uFMWcJTKaM

# Preprocessing
"""

import networkx as nx
import osmnx as ox

import pandas as pd
import numpy as np
import geopandas as gpd

df = pd.read_csv("geo_locations_astana_hackathon.csv")



df.head()

df.loc[df['spd'] < 0, 'spd'] = 0
df['spd'] = df['spd'] * 3.6

G = ox.graph_from_place("Astana, Kazakhstan", network_type="drive")
G_proj = ox.project_graph(G)

gdf = gpd.GeoDataFrame(
    df,
    geometry=gpd.points_from_xy(df.lng, df.lat),
    crs="EPSG:4326" # This is the WGS 84 CRS
)

gdf_proj = gdf.to_crs(G_proj.graph['crs'])

nearest_edges, dists = ox.nearest_edges(
    G_proj,
    X=gdf_proj.geometry.x.values,
    Y=gdf_proj.geometry.y.values,
    return_dist=True
)

df['distance_to_edge'] = dists
df['belongs_to'] = nearest_edges

df['edge_u'], df['edge_v'], df['edge_key'] = zip(*nearest_edges)

# Define your distance threshold in meters
threshold = 10  # meters

# Keep only the rows where the distance is within the threshold
matched_df = df[df['distance_to_edge'] < threshold].copy()

print(f"\nOriginal points: {len(df)}")
print(f"Points after matching (within {threshold}m): {len(matched_df)}")

matched_df.head()

# points_count_on_edges = matched_df.groupby('belongs_to').agg(
#     randomized_id_count=('randomized_id', 'count')
# ).sort_values(by='randomized_id_count', ascending=False)
points_count_on_edges = (
    matched_df
    .groupby('belongs_to')
    .agg(randomized_id_count=('randomized_id', 'nunique'))
    .sort_values(by='randomized_id_count', ascending=False)
)

import matplotlib.pyplot as plt

# Sort values so the biggest counts are on top
df_sorted = points_count_on_edges.sort_values("randomized_id_count", ascending=False)

plt.figure(figsize=(25,15))
plt.bar(range(len(df_sorted)), df_sorted["randomized_id_count"])
plt.xlabel("Edges (belongs_to)")
plt.ylabel("Count (randomized_id_count)")
plt.title("Counts per Edge")

# Optional: rotate x-axis labels if you want to show belongs_to values
# plt.xticks(range(len(df_sorted)), df_sorted.index, rotation=90)
plt.savefig('edges_histogram.png')
plt.show()

"""# Congestion calculation

If in the edge they are a lot of cars that are moving slow -> congestion. Cars - unique_randomized_id
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import minmax_scale # Using a standard library is often best practice

def compute_edge_stats(
    matched_df: pd.DataFrame,
    slow_speed_thresh: float = 10.0,
    dedupe_vehicle_pings: bool = False
) -> pd.DataFrame:
    """
    Computes and returns a DataFrame indexed by 'belongs_to' with key edge-level statistics.

    Args:
        matched_df: DataFrame with at least ['belongs_to', 'randomized_id', 'spd'].
        slow_speed_thresh: Speed below which a point is considered 'slow'.
        dedupe_vehicle_pings: If True, considers only the first ping from each vehicle on an edge.
                              This is useful if you want to count unique vehicles without multiple pings
                              from the same vehicle biasing other stats like mean speed.
    """
    df = matched_df.copy()
    df['spd'] = pd.to_numeric(df['spd'], errors='coerce')
    df = df.dropna(subset=['spd']) # Drop rows where speed is not a valid number

    if dedupe_vehicle_pings:
        # Keep only the first observation for each vehicle on each edge
        df = df.drop_duplicates(subset=['belongs_to', 'randomized_id'])

    # Perform all aggregations in a single, efficient operation
    edge_stats = df.groupby('belongs_to').agg(
        point_count=('spd', 'size'), # 'size' is faster than 'count' for non-null data
        unique_vehicles=('randomized_id', 'nunique'),
        mean_speed=('spd', 'mean'),
        median_speed=('spd', 'median'),
        speed_std=('spd', 'std'),
        # Calculate percentage of slow points directly and name the column
        pct_slow=('spd', lambda s: (s < slow_speed_thresh).mean())
    ).fillna(0) # Fill any resulting NaNs (e.g., std for a single point) with 0

    return edge_stats

def calculate_congestion_score(
    edge_stats: pd.DataFrame,
    w_volume: float = 0.1,
    w_slow: float = 0.9,
    w_speed: float = 0.0
) -> pd.Series:
    """
    Calculates a composite congestion score based on edge statistics.

    Args:
        edge_stats: DataFrame from compute_edge_stats.
        w_volume: Weight for the vehicle volume component.
        w_slow: Weight for the percentage of slow vehicles component.
        w_speed: Weight for the speed reduction component.
    """
    # 1. Volume Component: Log-transform to dampen effect of extreme outliers, then scale
    # Using 'unique_vehicles' is often more robust than 'point_count'
    scaled_volume = minmax_scale(np.log1p(edge_stats['unique_vehicles']))

    # 2. Slowness Component: This is already a percentage (0-1)
    pct_slow = edge_stats['pct_slow'].values

    # 3. Speed Reduction Component: Scaled median speed is more robust to outliers
    # We use (1 - speed) because lower speed means higher congestion
    scaled_speed_reduction = 1.0 - minmax_scale(edge_stats['median_speed'])

    # Combine into a weighted average
    raw_score = (
        w_volume * scaled_volume +
        w_slow * pct_slow +
        w_speed * scaled_speed_reduction
    )

    return pd.Series(raw_score, index=edge_stats.index, name='congestion_score')


def derive_congestion_for_edges(
    matched_df: pd.DataFrame,
    slow_speed_thresh: float = 12.0,
    composite_weights: tuple = (0.3, 0.5, 0.2)
) -> pd.DataFrame:
    """
    Top-level function to derive edge statistics and a composite congestion score.
    """
    stats = compute_edge_stats(matched_df, slow_speed_thresh=slow_speed_thresh)

    congestion_score = calculate_congestion_score(
        stats,
        w_volume=composite_weights[0],
        w_slow=composite_weights[1],
        w_speed=composite_weights[2]
    )

    # Join the score back to the stats DataFrame
    return stats.join(congestion_score)

# --- Example Usage ---
# Assuming 'matched_df' is your DataFrame
# edges = derive_congestion_for_edges(matched_df, slow_speed_thresh=12.0)
# print(edges.sort_values('congestion_score', ascending=False).head(10))





edges = derive_congestion_for_edges(matched_df, slow_speed_thresh=12.0, composite_weights=(0.55,0,0.45))
edges.sort_values('congestion_score', ascending=False).head(10)

edges.point_count.sum(), matched_df.shape[0]

matched_df.head()

north = df['lat'].max()
south = df['lat'].min()
east = df['lng'].max()
west = df['lng'].min()
bbox = (west, south, east, north)
G = ox.graph_from_bbox(bbox, network_type="drive")
G_proj = ox.project_graph(G)

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
import osmnx as ox

# G_proj is your projected graph and `edges` is your DataFrame with index 'belongs_to'
# make sure edges is a dataframe with columns ['belongs_to','congestion_score','point_count'] (point_count optional)
edges = edges.reset_index()  # if needed
cong_dict = dict(zip(edges['belongs_to'], edges['congestion_score']))
# if you also want linewidth scaling:
pc_dict = dict(zip(edges['belongs_to'], edges.get('point_count', np.ones(len(edges)))))

scores = []
edge_linewidths = []
# collect scores in the same order as graph edges iteration
for u, v, k, data in G_proj.edges(keys=True, data=True):
    # try exact orientation, then reversed orientation (if your DF used the opposite direction)
    score = cong_dict.get((u, v, k))
    if score is None:
        score = cong_dict.get((v, u, k))  # try reverse
    # fallback (if still None) -> np.nan (we'll color these gray)
    if score is None:
        scores.append(np.nan)
    else:
        scores.append(float(score))
    # optional linewidth by point_count (comment out if not needed)
    pc = pc_dict.get((u, v, k)) or pc_dict.get((v, u, k)) or 1
    edge_linewidths.append(1.0 + np.log1p(pc) * 0.6)  # example scaling

scores_arr = np.array(scores, dtype=float)

# Decide vmin/vmax (use percentiles to avoid extreme outliers dominating)
vmin = np.nanpercentile(scores_arr, 2)
vmax = np.nanpercentile(scores_arr, 98)
norm = colors.Normalize(vmin=vmin, vmax=vmax)
cmap = cm.get_cmap('Reds')

# create list of RGBA colors; missing scores -> light gray
edge_colors = [
    cmap(norm(s)) if not np.isnan(s) else (0.7, 0.7, 0.7, 1.0)
    for s in scores_arr
]

# Plot graph and keep figure/axes so we can add colorbar
fig, ax = ox.plot_graph(
    G_proj,
    edge_color=edge_colors,
    edge_linewidth=edge_linewidths,  # or use a single float like 1.2
    node_size=0,
    bgcolor='white',
    show=False,
    close=False
)

# create a ScalarMappable for the colorbar (use same cmap and norm)
sm = cm.ScalarMappable(norm=norm, cmap=cmap)
sm.set_array(scores_arr)  # required for colorbar scale

cbar = fig.colorbar(sm, ax=ax, fraction=0.03, pad=0.02)
cbar.set_label('congestion_score')

plt.show()
# if you prefer to save: fig.savefig('astana_congestion.png', dpi=300, bbox_inches='tight')

"""# Demand"""

# Define your distance threshold in meters
threshold = 10  # meters

# Keep only the rows where the distance is within the threshold
didar_df = df[df['distance_to_edge'] > threshold].copy()
didar_df = didar_df[didar_df['spd'] < 1]

print(f"\nOriginal points: {len(df)}")
print(f"Points after matching (within {threshold}m): {len(didar_df)}")

def haversine(lat1, lon1, lat2, lon2):
    R = 6371000  # Earth radius in meters
    phi1, phi2 = np.radians(lat1), np.radians(lat2)
    dphi = np.radians(lat2 - lat1)
    dlambda = np.radians(lon2 - lon1)

    a = np.sin(dphi/2.0)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dlambda/2.0)**2
    return 2 * R * np.arcsin(np.sqrt(a))

def points_within_radius(df, lat_a, lng_a, radius_m=100):
    distances = haversine(lat_a, lng_a, df['lat'].values, df['lng'].values)
    return df[distances <= radius_m]

# Example usage
lat_a, lng_a = 51.099, 71.422
result = points_within_radius(didar_df, lat_a, lng_a, radius_m=100)
result.shape

def scale_to_unit(x, min_val=0, max_val=3000):
    return (x - min_val) / (max_val - min_val)


def cost_function(data: pd.DataFrame, longitude: float, latitude: float, radius_m=100) -> float:
  coef = scale_to_unit(data.shape[0])

  return 200 * (np.exp(coef))

cost_function(result, lng_a, lat_a)



